{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LCBD Wiki","text":"<p>This website is dedicated to tutorials and information about LCBD software and analysis resources.</p> <p>If you're a current research participant or interested in enrolling in an LCBD study, visit childbrainlab.com.</p> <p>For information on how this site was automatically generated, visit mkdocs.org or view the README on the GitHub Repository.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Use the search tool, or go to any of the pages on the menu bar to find the help, documentation, or tutorial you're looking for. If you're a new member to the LCBD, visit the Setting Up docs.</p>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages.\n    assets/   # Images and other files.\nsite/         # Mkdocs build files, ignored in .gitignore on\n              # the main branch, this is the contents that are\n              # deployed to the gh-pages branch which this\n              # domain points to.\nREADME.md     # The README file shown on the GitHub repository page.\n</code></pre>"},{"location":"setting_up/CHPC/","title":"CHPC","text":"<p>In addition to the computational resources managed by NIL, we have access to resources managed by the Mallinckrodt Institute of Radiology's Center for High Performance Computing. Keep in mind - this is a computing system fully distinct from the NIL resources, including DynoSparky and Moochie, and is structurally an entirely different animal. Whereas DynoSparky is a self-contained server we explicitly use, CHPC is a computer cluster, meaning hundreds of computers are linked together to be used in parallel by many users running many different processes at once. Jobs are also not instantaneous, but rather submitted to a job queue, SLURM, and run in large batches concurrently, parellized so that, for example, each subject's processing has its own dedicated CPU. This is typically how large neuroimaging analysis jobs are  run in reasonable amounts of time. This level of parallelization is not possible on DynoSparky, though it is itself a powerful computer - HPC is like having a hundred DynoSparky's for a couple hours.</p> <p>The MIR's HPC system is dedicated to in-vivo imaging, and currently we are allowed access through the Psychiatry Department. To request an account, you'll need to have completed your HIPAA training through Learn@Work, after which you can submit an application here. Once accepted, familiarize yourself with the Rules and Guidelines for accessing CHPC - there are many more nuances to computing through this system, and you should not jump in without being highly familiar with Unix systems, the SLURM job queue, and awareness of the rescrictions and relevant quotas.</p> <p>For any lingering questions regarding CHPC, you can visit the FAQ page or email the director, Xing Huang, who has been extremely helpful and kind in assisting the lab with this resource.</p>"},{"location":"setting_up/CHPC/#connecting-to-chpc","title":"Connecting to CHPC","text":"<p>Once your account request has been accepted, you can access CHPC. To access via a 'login node' via SSH, you will use the following syntax:</p> <pre><code>ssh -Y &lt;wustl_key&gt;@login3.chpc.wustl.edu\n</code></pre> <p>and enter your WUSTL key password to complete authentication.</p> <p>Note: login3 is the alias for the \"3rd gen\" of login nodes used by CHPC. These will connect you \"round-robin\" to the two login nodes, login3-01 and login3-02. Use of login2 or login1 are deprecated and will lead you to encounter problems.</p> <p>To view your disk quotas on CHPC, enter the following command:</p> <pre><code>check_user_quota\n</code></pre> <p>To view the job queue, you can enter <code>squeue</code> or <code>squeue -u wustlID</code> to view only the jobs you've submitted.</p>"},{"location":"setting_up/DynoSparky/","title":"DynoSparky","text":"<p>Once users are approved by CSG, there are a few options to access DynoSparky:</p>"},{"location":"setting_up/DynoSparky/#mobaxterm-windows-only","title":"MobaXTerm (Windows Only)","text":"<p>MobaXterm is a graphical software which manages SSH sessions very nicely in Windows. It is a relatively simple alternative to remote desktop which saves computational resources, and is highly recommended for Windows users, with the main caveat being that copy/paste commands are not very intuitive. You can do either by right clicking. The default paste command is shift-insert, not ctrl-v as you may be used to. </p> <ol> <li> <p>To get started, launch MobaXterm.</p> <p></p> </li> <li> <p>Once launched, click the button in the top-left that says \"session\". </p> <p></p> </li> <li> <p>A new window will appear where you can enter the details of our session (these will be saved for later, you only need to do it once). Click the button in the new window that says \"SSH\" (secure-shell protocol). This ensures a safe connection between the local PC and DynoSparky. Note: if you're using this from home, you need to be connected to the WUSTL VPN. </p> <p></p> </li> <li> <p>Click \"OK\", and then fill in your user details as shown below. Ensure each of the highlighted areas matches, but use your username instead. The full remote host is <code>dynosparky.neuroimage.wustl.edu</code>. </p> <p></p> </li> <li> <p>Click \"OK\", and you should be prompted for your NIL password as usual. </p> <p></p> </li> </ol>"},{"location":"setting_up/DynoSparky/#remote-desktop","title":"Remote Desktop","text":"<p>Remote desktop takes advantage of Xubuntu to provide a graphical interface for navigating DynoSparky. This will be the most natural gateway for users who have not had experience navigating Linux systems via command prompt, or using SSH tunneling - however it is recommended that you spend time learning to use the terminal. Using remote desktop occupies much of the limited compute resources needlessly, and prevents our neuroimaging analyses from running quickly and in parallel. For some tasks and users, remote desktop is a sensible approach, and below are the instructions for using it in such situations.</p>"},{"location":"setting_up/DynoSparky/#for-mac-users","title":"For Mac Users","text":"<ol> <li>Download Microsoft Remote Desktop from the AppStore (either version 9 or 10).</li> <li>Once installed, open the App and click \"New\" at the top to set up a new connection.</li> <li> <p>For \"PC Name\", enter \"dynosparky.neuroimage.wustl.edu\". Under credentials, enter your NIL username (user camachoc is displayed below as an example). Note that this is NOT your WUSTL ID, this is your NIL login - they are separate credentials.</p> <p></p> </li> <li> <p>The other settings can be as you wish. The \"Connection Name\" is what the connection will be listed as in the App. You may choose to not leave your password entered, in which case you'll be prompted to enter it later. You may choose to check or uncheck the boxes at the bottom that determine how the window appears on your monitor.</p> </li> <li> <p>Close the Edit window. The new connection will now be listed in the main window of the app.</p> <p></p> </li> <li> <p>Double click the connection to start. The first time that you connect, the App will ask if you want to trust the server. Click yes.</p> </li> <li> <p>If you did NOT enter your password as part of the connection setup, you will see an error message. Click \"Ok\", then you will see the screen below. Log in with your NIL username and password.</p> <p></p> </li> <li> <p>Once you are logged in, you should see your desktop, and can proceed to the DynoSparky first-time setup documentation.</p> </li> </ol>"},{"location":"setting_up/DynoSparky/#for-windows-users","title":"For Windows Users","text":"<ol> <li> <p>Search for the \"Remote Desktop Connection\" default Windows application, and open it.</p> <p></p> </li> <li> <p>Once the window opens, click the \"Show Options\" down-arrow to expand the window. For Computer, enter <code>dynosparky.neuroimage.wustl.edu</code>. For User name, enter your NIL user name. Note that this is NOT the same as your WUSTL login.</p> <p></p> </li> <li> <p>If you are using a personal computer or account, you can opt to check the \"Allow me to save credentials\" box to reduce how often you have to log in. You can also click \"Save As\" and save the Remote Desktop Connection to your desktop to make it quicker to connect in the future.</p> </li> <li>Double-click your new connection or click \"connect\" from within the Remote Desktop Connection App. You will be prompted to log in with your NIL credentials. The first time you log in, you will get a message asking to allow connections with the remote computer, or if you can trust the certificate. For both, click \"Yes\" or \"Allow\". You can also check the \"Don't ask me about this again on this computer\" box to avoid having to give permission again later the next time you log in.</li> <li>Once you have logged in and provided permission to connect, you should see your desktop. If this is your first time connecting, you can proceed to the DynoSparky first-time setup documentation.</li> </ol>"},{"location":"setting_up/DynoSparky/#secure-shell-terminal-command-prompt","title":"Secure Shell (Terminal / Command Prompt)","text":"<p>Secure Shell (SSH) is a secure network protocol for operating services securely, even over unsecured networks. Even so, you must be connected to the Medical School VPN in order to connect.</p> <p>On any operating system, you can connect to DynoSparky using SSH.</p> <ol> <li>Ensure you are connected to the VPN</li> <li> <p>Open the Mac Terminal, Windows Command Prompt, or Linux Terminal</p> <p></p> </li> <li> <p>Type <code>ssh -Y NILUSER@dynosparky.neuroimage.wustl.edu</code> and hit ENTER. You should now be prompted to enter your password. Enter your password and hit enter (no asterisks will appear, like when you enter a password on a website). It may take a moment to connect.</p> <p></p> </li> <li> <p>If your OS is compatible with X11 forwarding, you may also use the <code>-X</code> flag to allow ssh to display graphical programs on your local computer. This is favored over using Remote Desktop, because the computational resources required to display the program are offloaded from DynoSparky, allowing the server to remain optimized in its role for multiple simultaneous connections. On Mac, you may need to first install XQuarts. E.g.,</p> </li> </ol> <pre><code>ssh -X NILUSER@dynosparky.neuroimage.wustl.edu\n</code></pre>"},{"location":"setting_up/DynoSparky/#setting-up-for-the-first-time-on-dynosparky","title":"Setting Up for the First Time on DynoSparky","text":"<p>The first time you log in, you'll need to make some modifications. It will be useful first to review the Command Line Help and Tips section. A great deal can be accomplished through the use of even just a few Bash commands, and it will save you time and energy to have them at your disposal.</p> <p>Each user on DynoSparky has a login-node space. This path is abbreviated for each user to <code>~</code>, and is equivalent to <code>/home/user/NILUSER.</code> This path is not necessarily your personal storage compartment (it is quite small), but is used for storing information about your DynoSparky configuration, and tools which may help you navigate the servers.</p>"},{"location":"setting_up/DynoSparky/#c-shell-configuration-file","title":"C Shell Configuration File","text":"<p><code>.cshrc</code> is a file that is executed each time you execute a new shell (i.e., each time you log in, open a new Remote Desktop session or Xterm window). .cshrc Docs</p> <p>The \".\" at the start of the filename indicates that it is a hidden file, a little-known but vastly important convention that allows terminal-users to interact with files not typically displayed in the file explorer window.</p> <p>LCBD members have compiled the most ubiquitous commands and shortcuts into a template .cshrc, which you can import to your own. The template cshrc file sets up the OS, as well as some PATH and environment variables which allow your user profile to recognize shared lab software packages, such as FSL and FreeSurfer. To merge it with your personal .cshrc (located in ~/.cshrc):</p> <ol> <li>Enter the following command:</li> </ol> <pre><code>echo source /data/perlman/moochie/user_data/cshrc.txt &gt;&gt; ~/.cshrc\n</code></pre> <p>This will append the contents of the template cshrc file to your personal configuration, <code>~/.cshrc</code>. NOTE: it is pertinent that you use <code>&gt;&gt;</code> and not <code>&gt;</code> (two carats). <code>&gt;&gt;</code> appends to a file, whereas <code>&gt;</code> writes the file out from scratch.</p> <ol> <li>After making changes to cshrc, they are not automatically loaded. Your csh configuratino file is only loaded once a new connection is made. Alternatively, you can force a refresh with:</li> </ol> <pre><code>source ~/.cshrc\n</code></pre>"},{"location":"setting_up/Moochie/","title":"Moochie","text":"<p>Moochie is the storage component of the lab's computational resources managed through CSG. Access will vary depending on your location:</p>"},{"location":"setting_up/Moochie/#off-campus","title":"Off Campus","text":"<p>Make sure you are connected to the VPN before proceeding! Once you are follow the steps below...</p>"},{"location":"setting_up/Moochie/#on-campus","title":"On Campus","text":"<p>On a lab PC (5th or 6th floor of 4444), once you are logged in with your NIL account:</p> <ol> <li>Open File Explorer</li> <li>Right click \"This PC\" on the sidebar</li> <li>Click \"Map network drive\"</li> <li>For \"Folder\" enter: \"\\10.20.145.33\\moochie\"</li> <li>Make sure \"reconnect at sign-in\" is checked</li> <li>Click \"Finish\"</li> <li>Login using the the username 'neuroimage\\nil-username' and your nil password</li> </ol> <p>Moochie will now display as a mounted network location on your lab PC. You will have to repeat this setup process on any new PCs you use.</p>"},{"location":"setting_up/Moochie/#on-mac","title":"On Mac","text":"<ol> <li>Open Finder</li> <li>Press command + K or click Go -&gt; Connect to server on menu bar</li> <li>Enter \"smb:\\10.20.145.33\\moochie\" and click \"Connect\"</li> <li>For \"Name\" enter your NIL username</li> <li>For \"Password\" enter your NIL password</li> <li>Click \"Connect\"</li> </ol> <p>To disconnect, click the eject button next to the server address on the Finder sidebar.</p>"},{"location":"setting_up/Moochie/#on-linux","title":"On Linux","text":"<pre><code>sudo sshfs -o allow_other NILUSER@dynosparky.neuroimage.wustl.edu:/data/perlman/moochie /path/to/mount/to\n</code></pre>"},{"location":"setting_up/setting_up/","title":"Basics","text":"<p>The lab uses two integrated servers, DynoSparky for processing and Moochie for storage. The server can only be accessed via the WUSTL network, so you must use the Medical School VPN to access DynoSparky from off campus.</p> <p>Moochie is mapped to a path, <code>/data/perlman/moochie</code>, on DynoSparky.</p> <p>DynoSparky runs the XFCE flavor of Ubuntu (a Linux OS distribution) known as Xubuntu. Any Ubuntu software will work just fine, but we also get the added benefit of XFCE's sleek interface, and graphical interfaces.</p> <p>Lab members do not have access to DynoSparky by default. Each user must be individually added, so ask Amanda if you need to be added.</p> <p>All of our computational resources (experiment and office PCs, DynoSparky, Moochie, VPN) are managed by the Computer Support Group, a sub-group of the Neuroimaging Labs Research Center. CSG can be contacted via email at nil-systems@npg.wustl.edu, but understand that they attend to nearly 70 different Linux systems, and are under extreme demand. Since the organization of NIL and CSG, as well as the resources they encompass can be a bit daunting, Timothy Brown was kind enough to produce Using Neuro-Imaging Laboratory (NIL) Resources to Store, Preprocess, and Analyze fMRI Data, which details the entire operation quite nicely - it's recommended that new lab members familiarize themselves with the resources discussed in this document, while also understanding that the exact usage of the computational resources may vary according to need within the LCBD.  </p>"},{"location":"tutorials/cnda_wt/","title":"CNDA MRI Pipeline","text":"<p>CNDA is the database to which the lab's MRI data is uploaded, immediately following its acquisition. It acts as a permanent storage base for the data, in its raw format, but is also capable of implementing automated preprocessing and analysis pipelines.   </p>"},{"location":"tutorials/cnda_wt/#xnat","title":"XNAT","text":"<p>CNDA's database is organized through descriptors (assessors) defined by the XNAT platform. XNAT is an extensible open-source imaging informatics software platform dedicated to imaging-based research.</p> <p>The scripts which we use to collect our MRI data from CNDA rely on the XNAT assessors and interface, in order to determine which data is required, which data is new, etc. </p>"},{"location":"tutorials/cnda_wt/#the-automated-cnda-moochie-pipeline","title":"The Automated CNDA -&gt; Moochie Pipeline","text":"<p>Pictured above is a simple depiction of the process for the automated CNDA data fetch which occurs automatically on Dynosparky, at intervals every hour from 6am to 8pm.</p> <p></p>"},{"location":"tutorials/cnda_wt/#cron","title":"Cron","text":"<p>The automation (repeated running) of commands on Linux servers such as Dynosparky can be achieved through the cron tool. Through instructions stored in a \"crontab\" file, the cron service will automatically execute the desired jobs at regular intervals. </p> <p>Accessing the crontab is as simple as running the following command:</p> <pre><code>crontab -e\n</code></pre> <p>Upon which you should see a template cron file resembling the following:</p> <pre><code># Edit this file to introduce tasks to be run by cron.\n#\n# Each task to run has to be defined through a single line\n# indicating with different fields when the task will be run\n# and what command to run for the task\n#\n# To define the time you can provide concrete values for\n# minute (m), hour (h), day of month (dom), month (mon),\n# and day of week (dow) or use '*' in these fields (for 'any').#\n# Notice that tasks will be started based on the cron's system\n# daemon's notion of time and timezones.\n#\n# Output of the crontab jobs (including errors) is sent through\n# email to the user the crontab file belongs to (unless redirected).\n#\n# For example, you can run a backup of all your user accounts\n# at 5 a.m every week with:\n# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/\n#\n# For more information see the manual pages of crontab(5) and cron(8)\n#\n# m h  dom mon dow   command\n</code></pre> <p>The two processes described in the figure can then be added to the cron scheduler with the following entries below the commented lines:</p> <pre><code># every hour from 6am-8pm, check for new CNDA sessions and email usability reminder\n0 6-20 * * * bash /data/perlman/moochie/github/LCBDtools/scripts/MRI/CNDA/CNDA_get_new_sessions.sh\n\n# every hour from 6am-8pm at :30, make a new download queue and download anything in it\n30 6-20 * * * bash /data/perlman/moochie/github/LCBDtools/scripts/MRI/CNDA/CNDA_download_queue.sh\n</code></pre> <p>The first entry runs at every minute equal to 0 (:00), hours ranging from 6-20 (6am - 8pm), at any (*) day, week, or month. The call to CNDA_get_new_sessions.sh executes a procedure that checks if there are any MR sessions on CNDA, which were not previously tracked. If it finds any, it will send an email to the LCBD account with a reminder to update the 'usability' field of each scan acquisition, on CNDA, and to thereafter paste the CNDA session ID into the data tracker, where it will be identified by the next step as ready-to-download.</p> <p>The second entry runs at every minute equal to 30 (:30), with the rest of the parameters unchanged. These were staggered to ensure that they don't overlap, since interfacing with CNDA twice simultaneously felt like a bad idea. The call to CNDA_download_queue.sh operates by looking for any previously-undownloaded CNDA session IDs, searching within the data tracker. If it finds one, it will execute the XNAT-reliant commands to download the usable acquisitions from the session, organize them within the MRI data folder, and merge any repeated sessions. Once the download has been completed, or if it did not download correctly, the program will send another automatic email to notify the lab staff that the scan is ready to use for brain photos, and to proceed with BIDS organizatio nand FMRIPREP. </p>"},{"location":"tutorials/cnda_wt/#dependencies","title":"Dependencies","text":"<p>Like most other code in this toolbox, any dependencies should be successfully resolved by running the following command:</p> <pre><code>source /data/perlman/moochie/resources/server_access/MRIenv/bin/activate\n</code></pre> <p>However, because of the extra credentials required for this pipeline, some additional solutions may be needed. </p>"},{"location":"tutorials/cnda_wt/#xnat-cnda-credentials","title":"XNAT / CNDA Credentials","text":"<p>For whomever is running the scripts (from their crontab, or manually), the user will need to establish a secure credentials file in their home directory. On Moochie, this would be found in <code>/home/usr/USERNAME</code>.</p> <p>To open and edit the correct (expected) file, enter the following command:</p> <pre><code>vim /home/usr/USERNAME/.xnatPass\n</code></pre> <p>and enter the following line, formatting the WUSTLID and PASSWORD fields with your WUSTL / CNDA credentials. Remove the brackets.</p> <pre><code>+&lt;WUSTLID&gt;@https://cnda.wustl.edu/REST=&lt;PASSWORD&gt;\n</code></pre> <p>After saving the file, we will now edit another CNDA credentials file:</p> <pre><code>vim /home/usr/USERNAME/.netrc\n</code></pre> <p>The format of the <code>.netrc</code> file is as follows:</p> <pre><code>machine cnda.wustl.edu\nlogin &lt;cnda_userid&gt;\npassword &lt;cnda_pw&gt;\nmachine https://cnda.wustl.edu\nlogin &lt;cnda_userid&gt;\npassword &lt;cnda_pw&gt;\n</code></pre> <p>Once this has been done successfully, you should be authorized to use the XNAT pipeline!</p>"},{"location":"tutorials/cnda_wt/#lcbdtools-email-credentials","title":"LCBDtools Email Credentials","text":"<p>In a similar configuration, the CNDA tools rely on an automated email service, authenticated through Google / Gmail. There may be additional steps required to verify the host to send emails, but ideally it is simpler than the xnatPass variant. Edit the designated filepath with the following command:</p> <pre><code>vim /home/usr/USERNAME/.email_creds\n</code></pre> <p>On the first line, you'll enter the email address: <code>LCBDtools@gmail.com</code>. </p> <p>On the next line, you will enter a one-time generated password from the Google App Password service. If this is new to you, you can see the instructions on this page. The actual password to the email account is the usual LCBD password, but for this service we need the one-time App Password, which you can generate by logging into the LCBDtools@gmail.com Google Account, selecting Security &gt; App Passwords &gt; Select app &gt; Select device &gt; Generate. Paste the generated password into the 2nd line of the <code>.email_creds</code> file. </p>"},{"location":"tutorials/cnda_wt/#protecting-your-credentials","title":"Protecting Your Credentials","text":"<p>Since you're storing your password in a plain-text file, you'll want to enable permissions on these files such that nobody else can read them. To protect the files, run these commands:</p> <p><code>chmod 600 /home/usr/USERNAME/.xnatPass</code></p> <p><code>chmod 600 /home/usr/USERNAME/.netrc</code></p> <p><code>chmod 600 /home/usr/USERNAME/.email_creds</code></p> <p>That's all! You should now be authenticated to run the entirety of the automated CNDA data retrieval procedures, automatically through the cron job scheduler.</p>"},{"location":"tutorials/linux_wt/","title":"Linux Intro and Command-Line Basics","text":""},{"location":"tutorials/linux_wt/#intro","title":"Intro","text":"<p>The Linux command line uses a variety of flavors of 'Unix', an operating system first developed in the 1960s. In general, the unix command format is as follows:</p> <pre><code>command argument1 argument2 --option1name option1value ...\n</code></pre> <p>E.g, you can use the <code>--help</code> option, which nearly every command has, to query the usage for the copy command, <code>cp</code>, with:</p> <pre><code>cp --help\n</code></pre> <p>You can think of every command like a shortcut - the actual functionality of the command is located in code elsewhere from your 'current location' on DynoSparky - but by evoking its name you request that code to be executed. If you are entirely new to Unix, you may find Andrew Jahn's Unix for Neuroimagers to be an extremely comprehensive resource. \"This will be a new language to you, and as with any language, the beginning can be disorienting and overwhelming at times; however, stick with it, and I promise that the fog will begin to clear eventually, and you will discern what exactly it is you need to know when searching the help manuals, and what it is that you can disregard.\"  </p>"},{"location":"tutorials/linux_wt/#cheat-sheet","title":"Cheat Sheet","text":"Command Description <code>ls</code> Lists all files and directories in the present working directory <code>ls -a</code> Lists hidden files as well <code>ls -l</code> Lists vertically with detailed information <code>ls -la</code> You can combine flags like so! <code>ls -l *</code> List each subfolder in a directory <code>ls -d /path/to/files/sub-*/ses-*/func</code> List the relative paths of all of the matching folders <code>cd ~</code> Navigate to $HOME directory <code>cd ..</code> Navigate one level up <code>cd &lt;path&gt;</code> Navigate to  <code>pwd</code> Prints the full path of the current directory <code>cat &lt;file&gt;</code> Prints the contents of a file <code>mv &lt;source&gt; &lt;destination&gt;</code> Move a source file or rename to destination <code>cp &lt;source&gt; &lt;destination&gt;</code> Copy a source file to destination <code>cp -r &lt;source&gt; &lt;destination&gt;</code> Copy a source folder to a destination <code>rm &lt;file&gt;</code> Permanently delete a file <code>rm -r &lt;folder&gt;</code> Permanently delete a folder <code>sudo</code> Placing 'sudo' in front of a command is like running as administrator <code>clear</code> Clears the terminal window <code>echo $VARIABLE</code> Displays value of variable <code>ssh -Y user@remoteaddress</code> Connect remotely to a server via secure shell <code>ssh -X -Y user@remoteaddress</code> Start SSH connection with X11 forwarding (graphical applications run locally) <code>touch &lt;filename&gt;</code> Create an empty file, at  <code>echo \"sometext\"</code> Print \"sometext\" to standard output <code>echo \"sometext\" &gt;&gt; &lt;filename&gt;</code> Append \"sometext\" to file <code>echo \"sometext\" &gt; &lt;filename&gt;</code> Write \"sometext\" from scratch to file <code>nano &lt;filename&gt;</code> Edit a file in the nano text editor <code>vim &lt;filename&gt;</code> Edit a file in the vim text editor <code>grep \"filterkey\"</code> Filters a standard output for lines containing \"filterkey\" <code>CTRL-c</code> Stop current command <p>These are just a few and some of the most powerful / common of many built-in commands.</p>"},{"location":"tutorials/linux_wt/#software-environment","title":"Software Environment","text":""},{"location":"tutorials/linux_wt/#c-shell-configuration-file","title":"C Shell Configuration File","text":"<p><code>.cshrc</code> is a file that is executed each time you execute a new shell (i.e., each time you log in, open a new Remote Desktop session or Xterm window). .cshrc Docs</p> <p>You can take advantage of <code>.cshrc</code> to built yourself shortcuts, aliases, and set up user configurations.</p> <p>E.g., if you're constantly using the command <code>ls -l</code> instead of regular <code>ls</code>, and it's becoming tedious to type in, you can add a line in your <code>.cshrc</code> which builds an alias for you:</p> <pre><code>alias l 'ls -l'\n</code></pre> <p>Then after either relaunching a session, or running <code>source ~/.cshrc</code> to load the file, you can use <code>l</code> in lieu of <code>ls -l</code>.</p>"},{"location":"tutorials/linux_wt/#virtual-environments","title":"Virtual Environments","text":""},{"location":"tutorials/linux_wt/#venv","title":"venv","text":"<p>One important environment we make use of in the LCBD is venv. Like other systems, such as anaconda, a virtual env</p>"},{"location":"tutorials/linux_wt/#anaconda","title":"anaconda","text":""},{"location":"tutorials/linux_wt/#docker","title":"Docker","text":"<p>Docker is a software we have access to on DynoSparky, which allows us to build, run, and store \"containers\". Many software for data analysis are offered via Docker containers, which are packaged applications that make the execution of the software much more straightforward and consistent.</p> <p>For example, when the latest releases of MRIQC, FMRIPREP, and LCBD scripts all rely on different versions of the PyDicom package, a Docker container works as a solution. In the figure below, Docker containers are represented on the left, and virutal machines on the right.</p> <p></p>"},{"location":"tutorials/mri_wt/","title":"fMRI Processing Pipeline","text":"<p>Once you have ensured the Automated CNDA Pipeline is set up, and you're pulling data successfully onto Moochie, you're ready to begin the steps to processing the fMRI data. </p>"},{"location":"tutorials/mri_wt/#data-management","title":"Data Management","text":"<p>Before getting into the full on pre-processing algorithms, there are some steps we take to ensure that data are managed properly, and transferred to the CHPC server.</p>"},{"location":"tutorials/mri_wt/#updating-moochies-sourcedata","title":"Updating Moochie's Sourcedata","text":"<p><code>sourcedata</code> is a folder within a BIDS repository that stores the raw, unaltered MRI data in DICOM format. On Moochie, this folder is located in <code>/data/perlman/moochie/analysis/STUDY_NAME/MRI_data_clean/sourcedata</code>. This data can be automatically organized here via an insertion of a command to your <code>crontab</code> file. Edit your <code>crontab</code> file with the following command:</p> <pre><code>crontab -e\n</code></pre> <p>And insert the following contents:</p> <pre><code># at 1 AM every Saturday, duplicate the NP1166 CNDA folder's contents (with some renaming) to the BIDS directory on Moochie\n0 1 * * 6 bash /data/perlman/moochie/github/LCBDtools/scripts/MRI/bids/update_sourcedata.sh CARE\n</code></pre>"},{"location":"tutorials/mri_wt/#syncing-the-sourcedata-folder-to-chpc","title":"Syncing the Sourcedata Folder to CHPC","text":"<p>We will also want this folder to be sync'd to the <code>sourcedata</code> folder on CHPC, where all of the subsequent processing will take hold. This too can be accomplished via a single-line insertion to your <code>crontab</code> file. </p> <p>Edit your <code>crontab</code> file with the following command:</p> <pre><code>crontab -e\n</code></pre> <p>And insert the following contents:</p> <pre><code># at 1 AM every Sunday, use 'rsync' to synchronize the data with the sourcedata folder on CHPC\n0 1 * * 7 bash /data/perlman/moochie/github/LCBDtools/scripts/MRI/transfer_CAREMRI_to_chpc.sh &gt;&gt; ~/transfer_subs_`data + '%m_%d_%Y'`.txt\n</code></pre> <p>The formatted data will now synchronize to CHPC every Sunday morning. </p>"},{"location":"tutorials/mri_wt/#bids-formatting","title":"BIDS Formatting","text":"<p>You'll first want a list of subjects who have yet to be formatted in the BIDS dataset. I.e., they have a session in <code>/sourcedata</code>, but that session isn't BIDS-formatted. </p> <p>There is a script for generating a list of un-BIDS'd subjects. To navigate to it, run this command:</p> <pre><code>cd /home/claytons/LCBDtools/scripts/MRI/bids\n</code></pre> <p>And run the following command:</p> <pre><code>python3 get_bids_participants.py --help\n</code></pre> <p>This will print information about the required and optional arguments for the script. To run,</p> <pre><code>python3 get_bids_participants.py --data_folder /scratch/claytons/MRI_data_clean\n</code></pre> <p>The script for submitting BIDSkit jobs on CHPC is located in <code>/home/claytons/LCBDtools/scripts/MRI/sbatch/bidskit_sbatch.sh</code>.</p> <p>To use the script, navigate to the sbatch directory with the following command:</p> <pre><code>cd /home/claytons/LCBDtools/scripts/MRI/sbatch\n</code></pre> <p>And submit a BIDSKIT job with the following command, substituting  with the intended subject number: <pre><code>sbatch bidskit_sbatch.sh /scratch/claytons/MRI_data_clean/ &lt;SUBJECT_ID&gt;\n</code></pre> <p>Or, alternatively, if you'd like to loop over all of the participants you generated in the previous step, you could use a bash loop, as follows:</p> <pre><code>for sub in `cat ~/bids_list.txt`; do sbatch bidskit_sbatch.sh /scratch/claytons/MRI_data_clean/ $sub; done\n</code></pre> <p>This will submit a job to CHPC. You can view activate jobs with the following command:</p> <pre><code>squeue -u USERID\n</code></pre>"},{"location":"tutorials/mri_wt/#preprocessing","title":"Preprocessing","text":"<p>Once all of the data management steps are completed, you should have a BIDS-formatted dataset. A subject's data will be converted to NIFTI from DICOM, and stored in the BIDS directory under <code>sub-&lt;SUBJECT_NUMBER&gt;</code>, with subfolders for <code>anat</code>, <code>func</code>, <code>dwi</code>, and any other modalities collected. A subject with complete BIDS data is ready to be preprocessed with FMRIPREP.</p>"},{"location":"tutorials/mri_wt/#generate-list-of-subjects-for-fmriprep","title":"Generate List of Subjects for FMRIPREP","text":"<p>Only subjects who have valid functional scans need to be run through FMRIPREP. To generate the list, navigate to the script directory:</p> <pre><code>cd /home/claytons/LCBDtools/scripts/MRI/fmriprep\n</code></pre> <p>And run the following command:</p> <pre><code>python3 get_fmriprep_participants.py --help\n</code></pre> <p>This will print information about the required and optional arguments for the script. To run,</p> <pre><code>python3 get_fmriprep_participants.py --data_folder /scratch/claytons/MRI_data_clean\n</code></pre> <p>This will generate a list of subject names that need to be run for FMRIPREP, saved to <code>~/participant_list.txt</code>. </p>"},{"location":"tutorials/mri_wt/#fmriprep","title":"FMRIPREP","text":"<p>Running FMRIPREP is very similar to running the BIDS conversion script. Navigate to the script directory with the following command:</p> <pre><code>cd /home/claytons/LCBDtools/scripts/MRI/sbatch\n</code></pre> <p>And submit FMRIPREP jobs with the following, substituting  with a valid subject ID: <pre><code>sbatch singularity_fmriprep_sbatch.sh /scratch/claytons/MRI_data_clean/ &lt;SUBJECT_NUMBER&gt;\n</code></pre> <p>Or, alternatively, if you'd like to loop over all of the participants you generated in the previous step, you could use a bash loop, as follows:</p> <pre><code>for sub in `cat ~/participant_list.txt`; do sbatch singularity_fmriprep_sbatch.sh /scratch/claytons/MRI_data_clean/ $sub; done\n</code></pre>"},{"location":"tutorials/mri_wt/#post-processing","title":"Post-Processing","text":"<p>The post-processing steps have been bundled into a single sbatch script, found in <code>LCBDtools/scripts/MRI/sbatch</code>, which accomplishes several tasks in one fell swoop.</p> <ul> <li>exporting regressors from the FMRIPREP confounds.txt file</li> <li>placing cropped versions of CARE movie-related regressors in the subjects' functional directories</li> <li>making motion spike regressors via the FSL tool, <code>fsl_motion_outliers</code></li> <li>smoothing the functional data with a 7mm gaussian kernel, and exporting in unarchived NIFTI format</li> </ul>"},{"location":"tutorials/mri_wt/#post-processing-batch","title":"Post-Processing Batch","text":"<p>To run the post-processing step, navigate to the sbatch directory:</p> <pre><code>cd /home/claytons/LCBDtools/scripts/MRI/sbatch\n</code></pre> <p>And run the following command:</p> <pre><code>sbatch post_processing_sbatch.sh /scratch/claytons/MRI_data_clean/\n</code></pre> <p>This will start quite a few jobs. The first steps are taken care of on a single node, but the smoothing and unarchiving will each occur on its own node. So, in reality, you're submitting as many jobs as there are subjects who need to be smoothed. This is noteworthy due to the 40 jobs / user cap on CHPC, meaning you may need to execute this command several times before all of the subjects have been thoroughly processed.</p>"},{"location":"tutorials/mri_wt/#evaluating-functional-runs-for-quality","title":"Evaluating Functional Runs for Quality","text":"<p>Now that FMRIPREP is complete, and we've run the post-processing script, we should evaluate the confounds file in the FMRIPREP output to ensure that subjects are not moving excessively. Similarly to the script which evaluates the subjects who need to be run through FMRIPREP, there is a Python script which can generate a list of valid subjects for us. First, navigate to the FSL directory:</p> <pre><code>cd /home/claytons/LCBDtools/scripts/MRI/fsl\n</code></pre> <p>Then, execute the script's help text with the following command:</p> <pre><code>python3 get_fsl_subs.py --help\n</code></pre> <p>Once you have read over the required and optional arguments, you can submit the evaluation with the following:</p> <pre><code>python3 get_fsl_subs.py --data_folder /scratch/claytons/MRI_data_clean\n</code></pre> <p>A list of subjects who pass the framewise-displacement motion criteria will be printed out to <code>~/fsl_subs.txt</code>. These are subjects who should be used in analysis moving forward. However, you should also evaluate the individual quality of the registration and T1 images using the <code>.html</code> output in the fmriprep derivatives folder, located at <code>/scratch/claytons/MRI_data_clean/derivatives/fmriprep</code>. Each subject has its own <code>.html</code> file, which can be opened in a web browser and inspected. It details information about the relevant confounds, displays images on the registration and field map interpretations, and will inform you of dropout and other MR artifacts. Any subjects with highly abnormal data shown here should also be excluded from analysis. </p>"},{"location":"tutorials/nirs_env_wt/","title":"fNIRS Environment Setup","text":"<p>The LCBD's fNIRS processing is performed on DynoSparky in a Jupyter Notebook (IPython) environment. If you haven't yet connected to DynoSparky, see the DynoSparky tutorial for steps on starting an SSH connection.</p> <p>This environment was selected for a few reasons:</p> <ul> <li>the computations happen on DynoSparky's resources, meaning your local PC or laptop won't be bogged down</li> <li>longer computations can run in the background on the Jupyter server for hours, or even days, without dropping</li> <li> <p>the implementation of a virtual environment means that every user can instantly acquire all of the dependency packages that fNIRS analysis relies on, such as:</p> </li> <li> <p>numpy</p> </li> <li>pandas</li> <li>MNE</li> <li>MNE-NIRS</li> <li>PyCWT</li> <li>and more</li> <li>moreover, the exact versions of these dependency packages are maintained across users ensuring consistent results</li> </ul> <p>The process for starting and connecting to a server is a bit lengthy, but is streamlined by the use of some helpful bash scripts. Note that you must be either connected to the VPN or on WUSTL internat. The full proceedings are as follows:</p> <ol> <li>Load the virtual environment, giving access to Jupyter and all of the dependencies</li> <li>Launch a Jupyter Notebook server (from DynoSparky) on a specific port, in the background using tmux</li> <li>Open an SSH tunnel to that port of DynoSparky on your local machine</li> <li>Visit the tunnel in a web page to direct Jupyter from your local machine</li> </ol> <p>To simplify this process, a single script, startJupyterServer.sh takes care of steps 1 and 2, and prints exact directions for steps 3 and 4.</p>"},{"location":"tutorials/nirs_env_wt/#starting-the-jupyter-server","title":"Starting the Jupyter Server","text":"<p>Once you have opened a terminal / SSH connection to DynoSparky navigate to <code>/data/perlman/moochie/github/LCBDtools</code> using the following command:</p> <pre><code>cd /data/perlman/moochie/github/LCBDtools\n</code></pre> <p>and subsequently run this command, inserting an appropriate port number (see below):</p> <pre><code>bash scripts/NIRS/startJupyterServer.sh &lt;port number&gt;\n</code></pre> <p>where the port number is an integer between 9000 and 9999. Ideally, each lab member should consistently use the same port number, since every Jupyter server needs its own, unused port number. Most numbers should be available, but it might be sensible to use something memorable to you, for example:</p> <ul> <li>9090</li> <li>9000</li> <li>9999</li> <li>9001</li> </ul>"},{"location":"tutorials/nirs_env_wt/#connecting-to-your-jupyter-server","title":"Connecting to Your Jupyter Server","text":"<p>If run successfully, script will output the exact details for the next steps. The DynoSparky side of things is done, and the SSH command it prints out is to be run on a local terminal, as shown below.</p> <p></p> <p>Once you've entered the SSH command it gives you, and authorized with your NIL password, you can open a web browser (any should be fine) and navigate to the URL, localhost:8888, where you should find the Jupyter web page. This is where you will enter the token passkey, if you haven't connected to the Jupyter server using this computer before. It will also give you the option to set a permanent password here, which you can reuse on any of your computers. </p> <p></p> <p>If this is your first time connecting from that computer, you may be prompted for a password. If so, you can find the token in the output of the startJupyterServer.sh script, in the URL, following the phrase, \"token=\". Copy this string of characters and paste it into the 'password' field.</p> <p>Note: you may need to tell Jupyter that you specifically want to use the NIRS environment (located at <code>/data/perlman/moochie/resources/server_access/NIRSenv</code>). When you have a notebook open, Jupyter tells you which kernel it is using in the upper right of the page:</p> <p></p> <p>You can click this to change the kernel to the NIRSenv so that you have access to all of the required dependencies.</p> <p></p> <p>You should now be good to proceed with your analysis.</p>"},{"location":"tutorials/nirs_env_wt/#ending-your-jupyter-session","title":"Ending Your Jupyter Session","text":"<p>If you're having problems with your server, forgot which port you're using, or need to restart the session for any reason, you can follow these steps to end your Jupyter server(s). </p> <ol> <li>Confirm the name of any of your tmux sessions using <code>tmux ls</code>. It will most likely be called <code>jupyter-server</code>, but depending on the steps you used to start, may also be simply a number, indexed starting at 0.</li> <li>Kill the tmux session with <code>tmux kill-session -t &lt;target&gt;</code>, replacing 'target' with your tmux session name from step 1. </li> <li>Repeat this for any tmux sessions you have that are running a Jupyter server. Visit the tmux cheat sheet for more information on tmux sessions. </li> </ol>"}]}